{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"text-align:right;\">Mario Stanke, University of Greifswald, Germany</p>\n",
    "\n",
    "# Exercise Set 1, Exercise 3 -  SGD Parameters\n",
    "\n",
    "Play with the parameters of the stochastic gradient descent algorithm.\n",
    " 1. Execute the notebook with the high-level method from this morning's lecture as is (Kernel -> Restart & Run All).\n",
    " 2. Jump to **Exercise** after the heading \"Solution 2\" and do as told there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  1482 \tn_cols =  2\n"
     ]
    }
   ],
   "source": [
    "# load the training data from the table file to a pandas data frame\n",
    "df = pd.read_csv(\"bikes-summerdays.tbl\", sep='\\s+')\n",
    "\n",
    "# convert count data to floats as regression target\n",
    "df['count'] = df['count'].astype(float) \n",
    "\n",
    "m, n_cols = df.shape # training set size and number of columns \n",
    "print(\"m = \", m, \"\\tn_cols = \", n_cols)\n",
    "\n",
    "# compute average of hourly bike rental counts\n",
    "meancount = np.mean(df['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Matrix $X$ and response vector $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract response vector\n",
    "y_train = np.array(df['count']) \n",
    "\n",
    "# extract feature columns\n",
    "n = n_cols - 1 # number of features\n",
    "temps = np.array(df.loc[:, df.columns != 'count']) # matrix of all other columns (here only one col)\n",
    "\n",
    "# make data matrix X\n",
    "X_train = np.ones((m, n+1)) # initialize with all ones\n",
    "# overwrite all but the zero-th column with features\n",
    "X_train[:,1:n+1] = temps / 10 - 2.5 # normalize temperatures so they are roughly in [-1,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error loss function\n",
    "def MSE(theta):\n",
    "    \"\"\"\n",
    "        Mean squared error function for linear regression\n",
    "        theta: numpy array of parameters, the first dimension must match the number of cols of X\n",
    "               If theta is 2-dimensional, the output is 1-dim with one entry per col of theta.\n",
    "    \"\"\"\n",
    "    if len(theta.shape) == 1: # theta is 1-dimensional\n",
    "        theta = tf.reshape(theta, (-1, 1)) # make it a matrix with one column\n",
    "    # now theta is a matrix in any case\n",
    "    yhat = tf.linalg.matmul(X_train, theta) # vector of predicted rental counts\n",
    "    d = (yhat - y_train.reshape((-1, 1)))**2 # square the residuals ('errors')\n",
    "    E = tf.reduce_sum(d, axis=0) / m\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single point: Tensor(\"truediv:0\", shape=(1,), dtype=float64)\n",
      "two points:  Tensor(\"truediv_1:0\", shape=(2,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# two tests of calls to error function\n",
    "print(\"single point:\", MSE(np.array([1., 2.])))\n",
    "print(\"two points: \", MSE(np.array([[1., 3.], [2., 4.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# this is for lecture purposes only and NEED NOT BE READ\n",
    "def contourPlot():\n",
    "    ''' Plot the error landscape'''\n",
    "    # compute error for the grid of all combinations of theta0 and theta1 values\n",
    "    theta0 = theta1 = np.arange(-3200., 3200., 100) # grid axis ranges\n",
    "    xv, yv = np.meshgrid(theta0, theta1) # x and y values of all grid points\n",
    "    Theta = np.array([xv, yv]).reshape(2, -1) # 2 rows, one col per grid point\n",
    "\n",
    "    # compute error for all grid points\n",
    "    z = MSE(Theta).numpy()\n",
    "    z = z.reshape((theta0.size, -1)) # make this a matrix again as required by contour\n",
    "\n",
    "    ## make contour plot\n",
    "    _, axC = plt.subplots()\n",
    "    # heights to draw contour lines for\n",
    "    h = [50000, 200000] + list(range(500000, 6000001, 500000))\n",
    "    contours = axC.contour(theta0, theta1, z, levels=h, colors='black')\n",
    "    axC.clabel(contours, inline=True, fontsize=8, fmt='%i') # labels on lines\n",
    "    plt.title(\"mean squared error \" + r'$E(\\theta)$');\n",
    "    plt.xlabel(r'$\\theta_0$')\n",
    "    plt.ylabel(r'$\\theta_1$');\n",
    "    return axC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of optimization progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(err, col):\n",
    "    ''' Plot error over time'''\n",
    "    plt.subplots()\n",
    "    plt.plot(np.log(err), 'o-', color=col, linewidth=.2, markersize=3, mfc='none')\n",
    "    plt.title(\"Log error by iteration of gradient descent\")\n",
    "    plt.xlabel('iteration '+r'$i$')\n",
    "    plt.ylabel(r'$ln \\;E(\\theta^{(i)})$');\n",
    "\n",
    "def plot_progress(Theta, err, col, ):\n",
    "    '''Plot error and parameters over time'''\n",
    "    plot_error(err, col)\n",
    "    # enter parameter trajectory to above contour plot\n",
    "    axC = contourPlot()\n",
    "    plt.plot(Theta[0, 0], Theta[1, 0], 'o', color='green', mfc='none') # mark starting point with circle\n",
    "    ymin, ymax = axC.get_ylim()\n",
    "    xmin, xmax = axC.get_xlim()\n",
    "    for i in range(1, Theta.shape[1]):\n",
    "        if (np.abs(Theta[0, i] - Theta[0, i-1]) + np.abs(Theta[1, i] - Theta[1, i-1]) > 1e-6):\n",
    "            # above condition avoids arrows with a length of 0 pixels\n",
    "            plt.arrow(Theta[0, i-1], Theta[1, i-1], Theta[0, i] - Theta[0, i-1], Theta[1, i] - Theta[1, i-1],\n",
    "                      color=col, width=3, head_length=20, head_width=50, overhang=.9, length_includes_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: high-level approach\n",
    "\n",
    "**Exercise:**  \n",
    "  1. Play with the *learning rate* $\\alpha$. Find a *small* value and a *large* value so that **convergence is not reached**.\n",
    "  2. Play with the *momentum* $\\mu$. Find a *small* value and a *large* value so that **convergence is not reached**.\n",
    "  3. Play with ```batch_size```. Find a *small* value so training is significantly **slower** and a *large* value so **convergence is not reached**.\n",
    "  \n",
    "When you vary one parameter, leave the other parameters as they were originally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic Option 1: Use the **keyboard**\n",
    "to edit the values in below cell, then **Run -> Run Selected Cell and Below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS CODE HERE for Option 1. If you choose Option 2, leave these values as defaults (0.01, 0.8 and 32, respectively)\n",
    "alpha = 0.01    # Solutions: ? and ?\n",
    "mu = 0.8        # Solutions: ? and ?\n",
    "batch_size = 32 # Solutions: ? and ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comfortable Option 2: Use the **mouse**\n",
    "Execute the next two cells, use the sliders to adjust the values, then run the rest of the cells.  \n",
    "*Tip:* **Create New View for Output** by right-clicking on the blue bar left of the graphs. Then drag the output window to a comfy place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "style = {'description_width': 'initial'}\n",
    "layout = widgets.Layout(width = '100%')\n",
    "alpha_slider = widgets.FloatLogSlider(value = alpha, min = -4, max = 1, step = 0.001,\n",
    "    description = 'learning rate ' + r'$\\alpha$:', readout_format = '.4f',\n",
    "    style = style, layout = layout)\n",
    "mu_slider = widgets.FloatSlider(value = mu, min = 0, max = 1.0, step = 0.02,\n",
    "    description = 'momentum ' + r'$\\mu$:', readout_format = '.2f',\n",
    "    style = style, layout = layout)\n",
    "batch_slider = widgets.IntSlider(value = batch_size, min = 1, max = m, step = 1,\n",
    "    description = 'batch size:', style = style, layout = layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83c043bd595499990c68f68ce138d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatLogSlider(value=0.01, description='learning rate $\\\\alpha$:', layout=Layout(width='100%'), max=1.0, min=-â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e89a003bac84f52b1456eedff2f9e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.8, description='momentum $\\\\mu$:', layout=Layout(width='100%'), max=1.0, step=0.02, style=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2ad9185ebc4f30a665666cc48e36f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=32, description='batch size:', layout=Layout(width='100%'), max=1482, min=1, style=SliderStyleâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(alpha_slider)\n",
    "display(mu_slider)\n",
    "display(batch_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = alpha_slider.value\n",
    "mu = mu_slider.value\n",
    "batch_size = batch_slider.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used values lambda = 0.01 mu= 0.8 batch_size= 32\n"
     ]
    }
   ],
   "source": [
    "print (\"Used values lambda =\", alpha, \"mu=\", mu, \"batch_size=\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate = alpha, momentum = mu)\n",
    "# SGD: stochastic gradient descent\n",
    "\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "# This common and preimplemented loss implements the MSE function defined in equation (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "- Heuristics for minimizing some error or loss function $E(\\pmb{\\theta}; ({\\bf X},{\\bf y}))$ with respect to $\\pmb{\\theta}$ more efficiently than with regular gradient descent (Formula (2)), in particular for large training data sets $({\\bf X},{\\bf y})$.\n",
    "- Also iteratively updates parameters $\\pmb{\\theta}$ on the basis of the gradient $\\nabla E$, but the gradient is computed only for a (small) sample $(Xbatch, Ybatch)$ of the training data. The number of examples in the batch are called the **batch size**.\n",
    "- The **learning rate** $\\alpha$ plays a similar role as in regular gradient descent: The larger $\\alpha$, the larger the changes to $\\pmb{\\theta}$ in each step.\n",
    "- $0\\le \\mu \\le 1$ is called **momentum**. If $\\mu = 0$, the update step is like in regular gradient descent, except that the gradient is computed on a batch only. With increasing $\\mu$ the update steps are larger and fluctuate less.\n",
    " > ${\\bf v} \\leftarrow {\\bf 0}$  \n",
    " > While the termination condition is not satisfied\n",
    " >> Sample a random batch $(\\text{Xbatch},\\text{Ybatch})$ from the training data $({\\bf X},{\\bf y})$.  \n",
    " >> ${\\bf v} \\leftarrow \\mu \\cdot {\\bf v} - \\alpha \\cdot \\nabla E(\\pmb{\\theta};\\text{Xbatch},\\text{Ybatch})$  \n",
    " >> $\\pmb{\\theta} \\leftarrow \\pmb{\\theta} + {\\bf v}$ \n",
    "  \n",
    "- The termination condition could be\n",
    "   1. A fixed number of iterations has been done.\n",
    "   2. The accuracy/error on a separate validation set satisfies some condition (e.g. it is plateauing).\n",
    "   3. User interruption.\n",
    "- Usually, the random choices of batches are such that the number of times each example appeared in a batch differs at most by 1, i.e. the batch is drawn from the subset of examples used hitherto one less than the maximum. Once all examples have been used once, an **epoch** ends.\n",
    "- Documentation and Code of [tf.keras.optimizers.SGD](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/SGD)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # make it reproducible\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# A tf.data.Dataset has commonly used functions for random sampling, obtaining subsets, transformations\n",
    "# iterating over large numbers of images on disk (using TFRecord).\n",
    "\n",
    "dataset = dataset.shuffle(m).batch(batch_size) # random order, use whole dataset as 'batch' for comparability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v1.initializers' has no attribute 'Constant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-51a2ce307caa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# bias equivalent to adding x_0 := 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# default would be random initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     dtype = 'float64')\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.initializers' has no attribute 'Constant'"
     ]
    }
   ],
   "source": [
    "# Get a predefined linear model with one single output variable (unit) and one weight per input.\n",
    "theta_init = np.array([2000., 3000.])\n",
    "model = tf.keras.Sequential()\n",
    "model = tf.keras.layers.Dense(\n",
    "    units = 1, input_dim = 2,\n",
    "    use_bias = False, # bias equivalent to adding x_0 := 1\n",
    "    kernel_initializer = tf.initializers.Constant(theta_init), # default would be random initialization\n",
    "    dtype = 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one gradient descent step\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        t = model(x) # predicted rental demand\n",
    "        t = tf.reshape(t, [-1])\n",
    "        E = loss_object(y, t)\n",
    "    grads = tape.gradient(E, model.trainable_variables)\n",
    "    # this makes a parameter update using the gradient\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "total_batches = N * math.ceil(m / batch_size)\n",
    "Theta3 = np.zeros((2, total_batches)) # sequence of parameters after each step\n",
    "err3 = np.zeros((total_batches)) # high-level approach error after each update step\n",
    "i = 0 # batch number\n",
    "\n",
    "for epoch in range(N):\n",
    "    # loop over batches of dataset\n",
    "    # Here it is here just a single big batch for comparability with Solution 1.\n",
    "    for (x, y) in dataset:\n",
    "        # x: batch features of shape (batch_size, 2)\n",
    "        # y: batch labels\n",
    "        E = train_step(x, y)\n",
    "        Theta3[:, [i]] = model.get_weights()[0]\n",
    "        err3[i] = E\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below line makes the plot interactive (zooming enabled) if uncommented but\n",
    "# works on jupyterhub.brain only for classic notebooks (Help -> Launch Classic Notebook).\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_progress(Theta3, err3, col = 'blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "  - In above plot of training error, the errors were measured only on a batch and are thus estimates of the training error only.\n",
    "  - When using stochastic gradient descent, typically the parameter trajectory eventually random walks near a local optimum and their latest values may result in a larger loss than some parameter value previously attained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final error =  27367.78125\n",
      "theta:\n",
      " [[326.95001808]\n",
      " [124.62979235]]\n"
     ]
    }
   ],
   "source": [
    "# final loss and prediction computed on all training data\n",
    "pred = model(X_train)\n",
    "pred = tf.reshape(pred, [m])\n",
    "E = loss_object(y_train, pred)\n",
    "print(\"final error = \", E.numpy())\n",
    "print(\"theta:\\n\", model.trainable_variables[0].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06254bff0aec4f75a0042060fdf3f6a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "0e89a003bac84f52b1456eedff2f9e04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "description": "momentum $\\mu$:",
       "layout": "IPY_MODEL_e10feb54a58a4b8c875a94e9a3b3b4c8",
       "max": 1,
       "step": 0.02,
       "style": "IPY_MODEL_924aa64300664104bc4a5e5a6387b37d",
       "value": 0.8
      }
     },
     "924aa64300664104bc4a5e5a6387b37d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "9551510913ca4ddb95ca8122dc13120a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "ae2ad9185ebc4f30a665666cc48e36f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "batch size:",
       "layout": "IPY_MODEL_e10feb54a58a4b8c875a94e9a3b3b4c8",
       "max": 1482,
       "min": 1,
       "style": "IPY_MODEL_06254bff0aec4f75a0042060fdf3f6a3",
       "value": 32
      }
     },
     "e10feb54a58a4b8c875a94e9a3b3b4c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "100%"
      }
     },
     "e83c043bd595499990c68f68ce138d1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatLogSliderModel",
      "state": {
       "description": "learning rate $\\alpha$:",
       "layout": "IPY_MODEL_e10feb54a58a4b8c875a94e9a3b3b4c8",
       "max": 1,
       "min": -4,
       "readout_format": ".4f",
       "step": 0.001,
       "style": "IPY_MODEL_9551510913ca4ddb95ca8122dc13120a",
       "value": 0.06039486293763797
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
