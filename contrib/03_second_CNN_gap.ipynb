{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The second CNN\n",
    "<b> Load the balanced and encoded data from the from the pickled DataFrame</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create a 80/20 train test split. You can use only the column with the one hot encoded sequences</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the second model\n",
    "The second model is an extension of the first model with some slight changes.<br>\n",
    "<li>A Conv1D layer with 16 filters of size 7</li>\n",
    "<li>A Max pooling layer with factor 2</li>\n",
    "<li>A Conv1D layer with 8 filters of size 7</li>\n",
    "<li>A Max pooling layer with factor 2</li>\n",
    "The remaining layers and properties are like the first model.<br>\n",
    "As before this should be a function that returns the compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "def create_second_model(input_shape, learning_rate=, verbose=):\n",
    "    # create the model\n",
    "   \n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model with one hot encoded sequences\n",
    "This time we are going to use some other features of the model.fit() function:\n",
    "<li>The fit() function has a parameter validation_split, which will split of some data to be use for validation during training. Set this to 0.2</li>\n",
    "<li>The fit() function returns an history object containing information about the training process</li>\n",
    "<li>We are going to train for 100 epochs with a batch_size of 32</li>\n",
    "<b> Create a KerasClassifier, train the model using the one hot encoded training data and save the training history.<br>\n",
    "Create a classification report and the MCC for the performance on the test data.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "epochs=\n",
    "batch_size=\n",
    "\n",
    "model = KerasClassifier(build_fn=create_second_model, input_shape=(), verbose=)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice when comparing this to our first model?<br>\n",
    "Let's have look at the training history.<br>\n",
    "<b>Create line plots for all metrics in the history using matplotlib.pyplot. What do you observe?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce overfitting\n",
    "It seems like our model might be overfitting a bit.<br>\n",
    "<b> Create another model (i.e a function returning a model) but this time include a Dropout layer with rate 0.2 just before the output layer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_second_model_dropout(input_shape, learning_rate=, dropout=, verbose=):\n",
    "    # create the model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Train and evaluate the model with dropout using the test data. Plot the training history.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "epochs=\n",
    "batch_size=\n",
    "\n",
    "\n",
    "plot_history(history_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV for learning and dropout rates\n",
    "The dropout seems to help against the overfitting. Explore the effects of different dropout rates and learning rates (an important hyper parameter) using GridSearchCV from scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).<br>\n",
    "<b>Search with learning rates = 0.001, 0.01, 0.05, 0.1 and a reasonable number of dropout rates between 0.1 and 0.9. Use 3 or 5 fold cross validation and score using the f1 score.<br>Print out the best score and the corresponding parameters</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = KerasClassifier(build_fn=create_second_model_dropout, input_shape=(), epochs=, verbose=)\n",
    "\n",
    "# define the grid search parameters\n",
    "rates = \n",
    "dropouts = \n",
    "\n",
    "param_grid = dict(learning_rate=rates, dropout=dropouts)\n",
    "\n",
    "grid = GridSearchCV()\n",
    "grid_result = \n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Print out the results (mean_test_score, std_test_score and corresponding params) for all parameter combinations</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = \n",
    "stds = \n",
    "params = \n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize gridsearch\n",
    "<b>To visualize the results of the grid search create a DataFrame with columns for the mean_test_scores, the epochs and the learning rates.<br> Then use the pivot_table function to create a matrix that can be visualized via a seaborn.heatmap</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "mean_scores = \n",
    "params = \n",
    "rates = \n",
    "dropouts = \n",
    "\n",
    "df_gridsearch = pd.DataFrame(zip(mean_scores, rates, dropouts), columns=[\"mean_test_score\",\"learning_rate\",\"dropout\"])\n",
    "df_grid_piv = df_gridsearch.pivot_table(index=\"learning_rate\", columns=\"dropout\").droplevel(axis=\"columns\", level=0)\n",
    "\n",
    "sns.heatmap(df_grid_piv, annot=True, cmap=\"Blues\"); #annot=True to annotate cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model\n",
    "<b>Train the model using the parameters found in the grid search. Plot all metrics in the training history and print out classification report and MCC for the test data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "epochs=\n",
    "batch_size=\n",
    "\n",
    "model = KerasClassifier(build_fn=create_second_model_dropout, input_shape=(), learning_rate=, dropout=, verbose=)\n",
    "\n",
    "\n",
    "yPred = model.predict()\n",
    "print(classification_report( yTest.to_list(), yPred ))\n",
    "print(\"MCC: \", matthews_corrcoef(yTest.to_list(), yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 fold cross validation\n",
    "For an assessment of the stability perform a 10-fold cross validation. Train for a maximum of 50 epochs.<br>\n",
    "<b>Collect the the training history, MCC, F1 score and accuracy from each round and calculate their mean values</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, accuracy_score\n",
    "\n",
    "epochs=\n",
    "splits=\n",
    "#to save scores\n",
    "df_scores = \n",
    "df_scores.index.name = \n",
    "\n",
    "# to save histories\n",
    "histories = []\n",
    "\n",
    "# define 10-fold cross validation test \n",
    "kfold = StratifiedKFold()\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(xTrain, yTrain)):\n",
    "    \n",
    "    # get the splits\n",
    "    X_train, X_test = \n",
    "    y_train, y_test = \n",
    "\n",
    "    # train one hot \n",
    "\n",
    "    \n",
    "    # save predictions\n",
    "    df_scores.loc[i] = [matthews_corrcoef(y_test.to_list(), yPred),\n",
    "                        f1_score(y_test.to_list(), yPred),\n",
    "                        accuracy_score(y_test.to_list(), yPred)]\n",
    "    # save history\n",
    "    histories.append(hist)\n",
    "    \n",
    "    print(\"Finished round {}\".format(i))\n",
    "    \n",
    "df_scores.loc['mean'] = df_scores.mean()\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Plot the training histories.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm with test data\n",
    "<b>Train the model without using a validation split and confirm the cross validation results on our test data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "epochs=\n",
    "batch_size=\n",
    "\n",
    "model = KerasClassifier(build_fn=create_second_model_dropout, input_shape=(), learning_rate=, verbose=)\n",
    "history_drop = model.fit()\n",
    "\n",
    "yPred = \n",
    "print(classification_report( yTest.to_list(), yPred ))\n",
    "print(\"MCC: \", matthews_corrcoef(yTest.to_list(), yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOTO 04 # it's all you from here on "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
