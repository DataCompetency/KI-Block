{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first CNN (but first more preparation)\n",
    "Unfortunately CNNs cannot work with letter sequences directly so we have to think about how to encode the sequences (and our labels) into numerical form, and although CNNs are able to deal with sequences of varying length this would require us to split the training set by length so we are going to pad the sequences to have a uniform length.<br>\n",
    "First we need our balanced dataset to continue.<br>\n",
    "<b>Read the balanced dataset from csv into a DataFrame</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_balanced = pd.read_csv(\"df_balanced.csv\", index_col=0)\n",
    "#df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASCII encoding and padding\n",
    "A simple idea to encode letters is to use their numerical representation according to the ASCII table.<br>\n",
    "<b>Create a new column in the DataFrame that contains the sequence as a list of decimal numbers according to the ASCII table</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced[\"seq_ord\"] = \n",
    "#df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the pad_sequences() function from keras.preprocessing to pad our sequences so they all have the same length (https://keras.io/preprocessing/sequence/).<br>\n",
    "<b> Use pad_sequences() to create a new column containing the padded ascii encoded sequences.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df_balanced[\"seq_ord_pad\"] = list(pad_sequences(df_balanced.seq_ord.to_numpy()))\n",
    "#df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another numerical encoding\n",
    "Since the ASCII representations can be large and also have varying distance between them, we want to reduce the encoding so that it uses only the smallest necessary integers. For this we can use the LabelEncoder from sklearn.preprocessing (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html ).<br>\n",
    "<b>Use the LabelEncoder to create a column containg the sequences in a minimal numerical encoding</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np # this might be handy\n",
    "\n",
    "seq_enc = LabelEncoder()\n",
    "seq_enc.fit()\n",
    "df_balanced[\"seq_num_pad\"] = df_balanced.seq_ord_pad.map()\n",
    "#print(list(df_balanced.seq_num_pad[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "A popular method for encdoding categorical features that avoids an implicit order is \"one hot encoding\". Again scikit-learn offers tools for this (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder ).<br>\n",
    "<b>Use OneHotEncoder to create column that contains the one hot encoded sequence (use sparse=False)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "oh_enc = OneHotEncoder()\n",
    "oh_enc.fit()\n",
    "\n",
    "df_balanced[\"seq_oh_pad\"] = df_balanced.seq_num_pad.map()\n",
    "\n",
    "#print(df_balanced.seq_oh_pad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattened one hot encoding\n",
    "This is a variation of the one hot encoding. Instead of having a one hot vector in each position the vectors are transposed and concatenated (i.e [[0,1],[1,0]] -> [0,1,1,0]).<br>\n",
    "<b>Create column that contains the flattened one hot encoded sequence</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced[\"seq_oh_flat\"] = df_balanced.seq_oh_pad.map()\n",
    "#df_balanced.seq_oh_flat[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget the labels\n",
    "We have to encode the labels as well. Again scikit-learn offers several options but we will just use the LabelEncoder again, since we only have two classes (i.e. 0 or 1).<br>\n",
    "<b>Create a column with encoded labels using LabelEncoder (save the encoder in a variable so we can use it for inverse transformation later)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lbl_enc = LabelEncoder()\n",
    "df_balanced[\"lbl_num\"] = lbl_enc.fit_transform()\n",
    "#df_balanced.lbl_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Save all of our hard work to a pickle so we can use it in other notebooks<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump(df_balanced, open(\"df_balanced_enc.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test\n",
    "<b>Create a 80/20 train test split, use the encoded labels!</b><br>\n",
    "You can fix the random seed to get a reproducible split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rnd_seed=42\n",
    "xTrain, xTest, yTrain, yTest = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create First convolutional model\n",
    "Now we are finally ready to create our first model.<br>\n",
    "The model is going to have the following simple architecture:\n",
    "<li>A 1D convolutional layer using an RelU activation with padding=\"same\" (Why?). Let's start with 16 filters of size 7. Use the input_shape parameter </li>\n",
    "<li>A max pooling layer reducing by a factor of 2</li>\n",
    "<li>A Flatten layer</li>\n",
    "<li>A Dense output layer using a sigmoid activation</li>\n",
    "<li>We will use the SGD optimizer with momentum set to 0.9. <i>What is a good choice for the loss function?</i></li><br>\n",
    "\n",
    "Since we want to use GridSearchCV from sklearn later on we will create a function that returns the compiled model (This is needed for the scikit-learn wraper).<br>\n",
    "This function should have parameters for the input_shape, because we are going to test our different encoding schemes.\n",
    "It should also have parameters for the number of filters and their sizes in the first convolutional layer for the grid search. <i>Feel free to insert a print(model.summary()) to get an overview of the model</i><br>We are going to train for 10 epochs using a batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "epochs=\n",
    "batch_size=\n",
    "\n",
    "def create_first_model(input_shape, opt=, c1_filter=, c1_size=, verbose=):\n",
    "    # create the model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    \n",
    "    model.compile()\n",
    "    \n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different encodings\n",
    "Now we will test the different encoding techniques with the first model. For each test the KerasClassifier wrapper will be used to create the model (https://keras.io/scikit-learn-api/ )\n",
    "<br>The model should then be trained with the training data and a classification report for the predictions on the test data should be generated. You can use th inverse_transform function of the LabelEncoder to have readable labels in the classification report.<br>Use the verbose option while fitting to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model with ASCII encoded sequences\n",
    "<b> Create a KerasClassifier, train the model using the ASCII encoded training data and evaluate the model (numpy.reshape might come in handy)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=, input_shape=, verbose=)\n",
    "model.fit(, , epochs=, batch_size=, verbose=)\n",
    "yPred = model.predict()\n",
    "print(classification_report( lbl_enc.inverse_transform(yTest.to_list()), lbl_enc.inverse_transform(yPred.ravel())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model with numerically encoded sequences\n",
    "<b> Create a KerasClassifier, train the model using the numerically encoded training data and evaluate the model (numpy.reshape might come in handy)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=, input_shape=, verbose=)\n",
    "model.fit(, , epochs=, batch_size=, verbose=) # start training\n",
    "yPred = model.predict()\n",
    "print(classification_report( yTest.to_list(), yPred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model with one hot encoded sequences\n",
    "<b> Create a KerasClassifier, train the model using the one hot encoded training data and evaluate the model (numpy.reshape might come in handy)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_first_model, input_shape=, verbose=)\n",
    "model.fit(, , epochs=, batch_size=, verbose=) # start training\n",
    "yPred = model.predict()\n",
    "print(classification_report( yTest.to_list(), yPred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model with flat one hot encoded sequences\n",
    "<b> Create a KerasClassifier, train the model using the flat one hot encoded training data and evaluate the model (numpy.reshape might come in handy)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=, input_shape=, verbose=)\n",
    "model.fit(, , epochs=, batch_size=, verbose=) # start training\n",
    "yPred = model.predict()\n",
    "print(classification_report( yTest.to_list(), yPred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Which encoding performed best?<br>\n",
    "Re-run the cells. Are the results stable?<br>\n",
    "Are all CNNs used the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embrace the randomness!\n",
    "In order to better evaluate the impact of the encoding we are going to perform a 5-fold cross validation. For this we are using the StratifiedKFold function from scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) and evaluate the Matthews Correlation Coefficient and the F1 score for each model in each round. <br>\n",
    "<b> Create a DataFrame with a column for each metric for each model.<br> \n",
    "    Create splits from the training dataset using StratifiedKFold.<br>\n",
    "    For each split train the model for all 4 encoding schemes and calculate the MCC for the test data (of the split).<br>\n",
    "    Create a new row for each round in the DataFrame and save the MCCs and F1 scores in the appropriate column<br>\n",
    "    Finally add a row to the DataFrame containing the mean for each model over all rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "\n",
    "#to save scores\n",
    "df_scores = pd.DataFrame(columns=[])\n",
    "df_scores.index.name = \"Round\"\n",
    "\n",
    "# define 10-fold cross validation test \n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(xTrain, yTrain)):\n",
    "    \n",
    "    # get the splits\n",
    "    X_train, X_test = \n",
    "    y_train, y_test = \n",
    "    \n",
    "    # train ascii model\n",
    "\n",
    "        \n",
    "    # train num model\n",
    "\n",
    "    \n",
    "    # train one hot \n",
    "\n",
    "    \n",
    "    # train flat one hot\n",
    "\n",
    "    \n",
    "    # save predictions\n",
    "    df_scores.loc[i] = []\n",
    "    \n",
    "    print(\"Finished round {}\".format(i))\n",
    "    \n",
    "df_scores.loc['mean'] = df_scores.mean()\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the encoding can have an impact on the performance of our model.<br>\n",
    "The one hot encoding seems to produce the best results so far, so we are going to concentrate on this encoding scheme when evaluating our next model.<br>\n",
    "The flattened one hot encoding also performed almost as good but it also increased the trainable parameters of our model (Why should that be a concern?)<br>\n",
    "<b>Let's move on to the next notebook \"03_second_CNN\" with a slightly more complex model<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
