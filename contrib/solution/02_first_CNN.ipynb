{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first CNN (but first more preparation)\n",
    "Unfortunately CNNs cannot work with letter sequences directly so we have to think about how to encode the sequences (and our labels) into numerical form, and although CNNs are able to deal with sequences of varying length this would require us to split the training set by length so we are going to pad the sequences to have a uniform length.<br>\n",
    "First we need our balanced dataset to continue.<br>\n",
    "<b>Read the balanced dataset from csv into a DataFrame</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seq</th>\n",
       "      <th>Label</th>\n",
       "      <th>Length</th>\n",
       "      <th>GC_content</th>\n",
       "      <th>ATGC_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Arabidopsis_thaliana300001_SnoR1b</th>\n",
       "      <td>GGCGAGGATGAATAATGCTAAATTTCTGACACCTCTTGTATGAGGA...</td>\n",
       "      <td>CD-box</td>\n",
       "      <td>93</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>1.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabidopsis_thaliana300003_SnoR10-1</th>\n",
       "      <td>AGAAATGATGAGAAATCAGATAAATCTTAGGACACCTTCTGACACA...</td>\n",
       "      <td>CD-box</td>\n",
       "      <td>81</td>\n",
       "      <td>0.345679</td>\n",
       "      <td>1.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabidopsis_thaliana300006_SnoR101</th>\n",
       "      <td>GGGATACACTTGATCTCTGAACTTCACAGGTAAGTTCGCTTGTTGA...</td>\n",
       "      <td>CD-box</td>\n",
       "      <td>68</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>1.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabidopsis_thaliana300007_SnoR102</th>\n",
       "      <td>AGAAGTCAATAGACCAGACATTGTGGTAACACTCTCTTTCATGGCA...</td>\n",
       "      <td>CD-box</td>\n",
       "      <td>133</td>\n",
       "      <td>0.413534</td>\n",
       "      <td>1.418182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabidopsis_thaliana300010_SnoR105</th>\n",
       "      <td>AGGGGATATGATGAATGGTAAAAACTCGCTTATATTGCGAGAAGAG...</td>\n",
       "      <td>CD-box</td>\n",
       "      <td>107</td>\n",
       "      <td>0.448598</td>\n",
       "      <td>1.229167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saccharomyces_cerevisiae300063_snR35</th>\n",
       "      <td>ATACAAAATTAATCGTGCGGATTAATAATCCAGGACTATAAAACCG...</td>\n",
       "      <td>HACA-box</td>\n",
       "      <td>204</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>1.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saccharomyces_cerevisiae300064_snR5</th>\n",
       "      <td>ATCATTCAATAAACTGATCTTCCGGATTACCATGCTTAAGACATCA...</td>\n",
       "      <td>HACA-box</td>\n",
       "      <td>204</td>\n",
       "      <td>0.328431</td>\n",
       "      <td>2.044776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saccharomyces_cerevisiae300065_snR9</th>\n",
       "      <td>GGGAATATAATACTAAATACTCTGTTATATAGAACTTTCTACGCCT...</td>\n",
       "      <td>HACA-box</td>\n",
       "      <td>187</td>\n",
       "      <td>0.379679</td>\n",
       "      <td>1.633803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saccharomyces_cerevisiae300070_snR44</th>\n",
       "      <td>CTCCGGGCTGATAACTAGATGGTGTGATCGGGCAGTATACTAATTT...</td>\n",
       "      <td>HACA-box</td>\n",
       "      <td>211</td>\n",
       "      <td>0.388626</td>\n",
       "      <td>1.573171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saccharomyces_cerevisiae300073_snR191</th>\n",
       "      <td>TACCAAACCTTTTTGTCAGGGTGCTTCTCTATCCGTTTTAGGATAA...</td>\n",
       "      <td>HACA-box</td>\n",
       "      <td>274</td>\n",
       "      <td>0.383212</td>\n",
       "      <td>1.609524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Seq  \\\n",
       "Arabidopsis_thaliana300001_SnoR1b      GGCGAGGATGAATAATGCTAAATTTCTGACACCTCTTGTATGAGGA...   \n",
       "Arabidopsis_thaliana300003_SnoR10-1    AGAAATGATGAGAAATCAGATAAATCTTAGGACACCTTCTGACACA...   \n",
       "Arabidopsis_thaliana300006_SnoR101     GGGATACACTTGATCTCTGAACTTCACAGGTAAGTTCGCTTGTTGA...   \n",
       "Arabidopsis_thaliana300007_SnoR102     AGAAGTCAATAGACCAGACATTGTGGTAACACTCTCTTTCATGGCA...   \n",
       "Arabidopsis_thaliana300010_SnoR105     AGGGGATATGATGAATGGTAAAAACTCGCTTATATTGCGAGAAGAG...   \n",
       "...                                                                                  ...   \n",
       "Saccharomyces_cerevisiae300063_snR35   ATACAAAATTAATCGTGCGGATTAATAATCCAGGACTATAAAACCG...   \n",
       "Saccharomyces_cerevisiae300064_snR5    ATCATTCAATAAACTGATCTTCCGGATTACCATGCTTAAGACATCA...   \n",
       "Saccharomyces_cerevisiae300065_snR9    GGGAATATAATACTAAATACTCTGTTATATAGAACTTTCTACGCCT...   \n",
       "Saccharomyces_cerevisiae300070_snR44   CTCCGGGCTGATAACTAGATGGTGTGATCGGGCAGTATACTAATTT...   \n",
       "Saccharomyces_cerevisiae300073_snR191  TACCAAACCTTTTTGTCAGGGTGCTTCTCTATCCGTTTTAGGATAA...   \n",
       "\n",
       "                                          Label  Length  GC_content  \\\n",
       "Arabidopsis_thaliana300001_SnoR1b        CD-box      93    0.419355   \n",
       "Arabidopsis_thaliana300003_SnoR10-1      CD-box      81    0.345679   \n",
       "Arabidopsis_thaliana300006_SnoR101       CD-box      68    0.441176   \n",
       "Arabidopsis_thaliana300007_SnoR102       CD-box     133    0.413534   \n",
       "Arabidopsis_thaliana300010_SnoR105       CD-box     107    0.448598   \n",
       "...                                         ...     ...         ...   \n",
       "Saccharomyces_cerevisiae300063_snR35   HACA-box     204    0.392157   \n",
       "Saccharomyces_cerevisiae300064_snR5    HACA-box     204    0.328431   \n",
       "Saccharomyces_cerevisiae300065_snR9    HACA-box     187    0.379679   \n",
       "Saccharomyces_cerevisiae300070_snR44   HACA-box     211    0.388626   \n",
       "Saccharomyces_cerevisiae300073_snR191  HACA-box     274    0.383212   \n",
       "\n",
       "                                       ATGC_ratio  \n",
       "Arabidopsis_thaliana300001_SnoR1b        1.384615  \n",
       "Arabidopsis_thaliana300003_SnoR10-1      1.892857  \n",
       "Arabidopsis_thaliana300006_SnoR101       1.266667  \n",
       "Arabidopsis_thaliana300007_SnoR102       1.418182  \n",
       "Arabidopsis_thaliana300010_SnoR105       1.229167  \n",
       "...                                           ...  \n",
       "Saccharomyces_cerevisiae300063_snR35     1.550000  \n",
       "Saccharomyces_cerevisiae300064_snR5      2.044776  \n",
       "Saccharomyces_cerevisiae300065_snR9      1.633803  \n",
       "Saccharomyces_cerevisiae300070_snR44     1.573171  \n",
       "Saccharomyces_cerevisiae300073_snR191    1.609524  \n",
       "\n",
       "[984 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_balanced = pd.read_csv(\"df_balanced.csv\", index_col=0)\n",
    "df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASCII encoding and padding\n",
    "A simple idea to encode letters is to use their numerical representation according to the ASCII table.<br>\n",
    "<b>Create a new column in the DataFrame that contains the sequence as a list of decimal numbers according to the ASCII table</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced[\"seq_ord\"] = df_balanced.Seq.map(lambda x: [ord(c) for c in x])\n",
    "#df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the pad_sequences() function from keras.preprocessing to pad our sequences so they all have the same length (https://keras.io/preprocessing/sequence/).<br>\n",
    "<b> Use pad_sequences() to create a new column containing the padded ascii encoded sequences.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seq</th>\n",
       "      <th>Label</th>\n",
       "      <th>Length</th>\n",
       "      <th>GC_content</th>\n",
       "      <th>ATGC_ratio</th>\n",
       "      <th>seq_ord</th>\n",
       "      <th>seq_ord_pad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Arabidopsis_thaliana300001_SnoR1b</th>\n",
       "      <td>GGCGAGGATGAATAATGCTAAATTTCTGACACCTCTTGTATGAGGA...</td>\n",
       "      <td>CD-box</td>\n",
       "      <td>93</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>[71, 71, 67, 71, 65, 71, 71, 65, 84, 71, 65, 6...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabidopsis_thaliana300003_SnoR10-1</th>\n",
       "      <td>AGAAATGATGAGAAATCAGATAAATCTTAGGACACCTTCTGACACA...</td>\n",
       "      <td>CD-box</td>\n",
       "      <td>81</td>\n",
       "      <td>0.345679</td>\n",
       "      <td>1.892857</td>\n",
       "      <td>[65, 71, 65, 65, 65, 84, 71, 65, 84, 71, 65, 7...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabidopsis_thaliana300006_SnoR101</th>\n",
       "      <td>GGGATACACTTGATCTCTGAACTTCACAGGTAAGTTCGCTTGTTGA...</td>\n",
       "      <td>CD-box</td>\n",
       "      <td>68</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>[71, 71, 71, 65, 84, 65, 67, 65, 67, 84, 84, 7...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabidopsis_thaliana300007_SnoR102</th>\n",
       "      <td>AGAAGTCAATAGACCAGACATTGTGGTAACACTCTCTTTCATGGCA...</td>\n",
       "      <td>CD-box</td>\n",
       "      <td>133</td>\n",
       "      <td>0.413534</td>\n",
       "      <td>1.418182</td>\n",
       "      <td>[65, 71, 65, 65, 71, 84, 67, 65, 65, 84, 65, 7...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabidopsis_thaliana300010_SnoR105</th>\n",
       "      <td>AGGGGATATGATGAATGGTAAAAACTCGCTTATATTGCGAGAAGAG...</td>\n",
       "      <td>CD-box</td>\n",
       "      <td>107</td>\n",
       "      <td>0.448598</td>\n",
       "      <td>1.229167</td>\n",
       "      <td>[65, 71, 71, 71, 71, 65, 84, 65, 84, 71, 65, 8...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saccharomyces_cerevisiae300063_snR35</th>\n",
       "      <td>ATACAAAATTAATCGTGCGGATTAATAATCCAGGACTATAAAACCG...</td>\n",
       "      <td>HACA-box</td>\n",
       "      <td>204</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>[65, 84, 65, 67, 65, 65, 65, 65, 84, 84, 65, 6...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saccharomyces_cerevisiae300064_snR5</th>\n",
       "      <td>ATCATTCAATAAACTGATCTTCCGGATTACCATGCTTAAGACATCA...</td>\n",
       "      <td>HACA-box</td>\n",
       "      <td>204</td>\n",
       "      <td>0.328431</td>\n",
       "      <td>2.044776</td>\n",
       "      <td>[65, 84, 67, 65, 84, 84, 67, 65, 65, 84, 65, 6...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saccharomyces_cerevisiae300065_snR9</th>\n",
       "      <td>GGGAATATAATACTAAATACTCTGTTATATAGAACTTTCTACGCCT...</td>\n",
       "      <td>HACA-box</td>\n",
       "      <td>187</td>\n",
       "      <td>0.379679</td>\n",
       "      <td>1.633803</td>\n",
       "      <td>[71, 71, 71, 65, 65, 84, 65, 84, 65, 65, 84, 6...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saccharomyces_cerevisiae300070_snR44</th>\n",
       "      <td>CTCCGGGCTGATAACTAGATGGTGTGATCGGGCAGTATACTAATTT...</td>\n",
       "      <td>HACA-box</td>\n",
       "      <td>211</td>\n",
       "      <td>0.388626</td>\n",
       "      <td>1.573171</td>\n",
       "      <td>[67, 84, 67, 67, 71, 71, 71, 67, 84, 71, 65, 8...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saccharomyces_cerevisiae300073_snR191</th>\n",
       "      <td>TACCAAACCTTTTTGTCAGGGTGCTTCTCTATCCGTTTTAGGATAA...</td>\n",
       "      <td>HACA-box</td>\n",
       "      <td>274</td>\n",
       "      <td>0.383212</td>\n",
       "      <td>1.609524</td>\n",
       "      <td>[84, 65, 67, 67, 65, 65, 65, 67, 67, 84, 84, 8...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Seq  \\\n",
       "Arabidopsis_thaliana300001_SnoR1b      GGCGAGGATGAATAATGCTAAATTTCTGACACCTCTTGTATGAGGA...   \n",
       "Arabidopsis_thaliana300003_SnoR10-1    AGAAATGATGAGAAATCAGATAAATCTTAGGACACCTTCTGACACA...   \n",
       "Arabidopsis_thaliana300006_SnoR101     GGGATACACTTGATCTCTGAACTTCACAGGTAAGTTCGCTTGTTGA...   \n",
       "Arabidopsis_thaliana300007_SnoR102     AGAAGTCAATAGACCAGACATTGTGGTAACACTCTCTTTCATGGCA...   \n",
       "Arabidopsis_thaliana300010_SnoR105     AGGGGATATGATGAATGGTAAAAACTCGCTTATATTGCGAGAAGAG...   \n",
       "...                                                                                  ...   \n",
       "Saccharomyces_cerevisiae300063_snR35   ATACAAAATTAATCGTGCGGATTAATAATCCAGGACTATAAAACCG...   \n",
       "Saccharomyces_cerevisiae300064_snR5    ATCATTCAATAAACTGATCTTCCGGATTACCATGCTTAAGACATCA...   \n",
       "Saccharomyces_cerevisiae300065_snR9    GGGAATATAATACTAAATACTCTGTTATATAGAACTTTCTACGCCT...   \n",
       "Saccharomyces_cerevisiae300070_snR44   CTCCGGGCTGATAACTAGATGGTGTGATCGGGCAGTATACTAATTT...   \n",
       "Saccharomyces_cerevisiae300073_snR191  TACCAAACCTTTTTGTCAGGGTGCTTCTCTATCCGTTTTAGGATAA...   \n",
       "\n",
       "                                          Label  Length  GC_content  \\\n",
       "Arabidopsis_thaliana300001_SnoR1b        CD-box      93    0.419355   \n",
       "Arabidopsis_thaliana300003_SnoR10-1      CD-box      81    0.345679   \n",
       "Arabidopsis_thaliana300006_SnoR101       CD-box      68    0.441176   \n",
       "Arabidopsis_thaliana300007_SnoR102       CD-box     133    0.413534   \n",
       "Arabidopsis_thaliana300010_SnoR105       CD-box     107    0.448598   \n",
       "...                                         ...     ...         ...   \n",
       "Saccharomyces_cerevisiae300063_snR35   HACA-box     204    0.392157   \n",
       "Saccharomyces_cerevisiae300064_snR5    HACA-box     204    0.328431   \n",
       "Saccharomyces_cerevisiae300065_snR9    HACA-box     187    0.379679   \n",
       "Saccharomyces_cerevisiae300070_snR44   HACA-box     211    0.388626   \n",
       "Saccharomyces_cerevisiae300073_snR191  HACA-box     274    0.383212   \n",
       "\n",
       "                                       ATGC_ratio  \\\n",
       "Arabidopsis_thaliana300001_SnoR1b        1.384615   \n",
       "Arabidopsis_thaliana300003_SnoR10-1      1.892857   \n",
       "Arabidopsis_thaliana300006_SnoR101       1.266667   \n",
       "Arabidopsis_thaliana300007_SnoR102       1.418182   \n",
       "Arabidopsis_thaliana300010_SnoR105       1.229167   \n",
       "...                                           ...   \n",
       "Saccharomyces_cerevisiae300063_snR35     1.550000   \n",
       "Saccharomyces_cerevisiae300064_snR5      2.044776   \n",
       "Saccharomyces_cerevisiae300065_snR9      1.633803   \n",
       "Saccharomyces_cerevisiae300070_snR44     1.573171   \n",
       "Saccharomyces_cerevisiae300073_snR191    1.609524   \n",
       "\n",
       "                                                                                 seq_ord  \\\n",
       "Arabidopsis_thaliana300001_SnoR1b      [71, 71, 67, 71, 65, 71, 71, 65, 84, 71, 65, 6...   \n",
       "Arabidopsis_thaliana300003_SnoR10-1    [65, 71, 65, 65, 65, 84, 71, 65, 84, 71, 65, 7...   \n",
       "Arabidopsis_thaliana300006_SnoR101     [71, 71, 71, 65, 84, 65, 67, 65, 67, 84, 84, 7...   \n",
       "Arabidopsis_thaliana300007_SnoR102     [65, 71, 65, 65, 71, 84, 67, 65, 65, 84, 65, 7...   \n",
       "Arabidopsis_thaliana300010_SnoR105     [65, 71, 71, 71, 71, 65, 84, 65, 84, 71, 65, 8...   \n",
       "...                                                                                  ...   \n",
       "Saccharomyces_cerevisiae300063_snR35   [65, 84, 65, 67, 65, 65, 65, 65, 84, 84, 65, 6...   \n",
       "Saccharomyces_cerevisiae300064_snR5    [65, 84, 67, 65, 84, 84, 67, 65, 65, 84, 65, 6...   \n",
       "Saccharomyces_cerevisiae300065_snR9    [71, 71, 71, 65, 65, 84, 65, 84, 65, 65, 84, 6...   \n",
       "Saccharomyces_cerevisiae300070_snR44   [67, 84, 67, 67, 71, 71, 71, 67, 84, 71, 65, 8...   \n",
       "Saccharomyces_cerevisiae300073_snR191  [84, 65, 67, 67, 65, 65, 65, 67, 67, 84, 84, 8...   \n",
       "\n",
       "                                                                             seq_ord_pad  \n",
       "Arabidopsis_thaliana300001_SnoR1b      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "Arabidopsis_thaliana300003_SnoR10-1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "Arabidopsis_thaliana300006_SnoR101     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "Arabidopsis_thaliana300007_SnoR102     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "Arabidopsis_thaliana300010_SnoR105     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                                                  ...  \n",
       "Saccharomyces_cerevisiae300063_snR35   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "Saccharomyces_cerevisiae300064_snR5    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "Saccharomyces_cerevisiae300065_snR9    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "Saccharomyces_cerevisiae300070_snR44   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "Saccharomyces_cerevisiae300073_snR191  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[984 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df_balanced[\"seq_ord_pad\"] = list(pad_sequences(df_balanced.seq_ord.to_numpy()))\n",
    "df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another numerical encoding\n",
    "Since the ASCII representations can be large and also have varying distance between them, we want to reduce the encoding so that it uses only the smallest necessary integers. For this we can use the LabelEncoder from sklearn.preprocessing (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html ).<br>\n",
    "<b>Use the LabelEncoder to create a column containg the sequences in a minimal numerical encoding</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 2, 3, 1, 3, 3, 1, 4, 3, 1, 1, 4, 1, 1, 4, 3, 2, 4, 1, 1, 1, 4, 4, 4, 2, 4, 3, 1, 2, 1, 2, 2, 4, 2, 4, 4, 3, 4, 1, 4, 3, 1, 3, 3, 1, 3, 1, 3, 1, 4, 4, 3, 1, 4, 1, 1, 2, 2, 4, 2, 4, 2, 2, 4, 4, 4, 3, 1, 3, 2, 1, 2, 1, 4, 4, 1, 4, 3, 2, 1, 1, 4, 1, 2, 4, 2, 4, 3, 1, 3, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np # this might be handy\n",
    "\n",
    "seq_enc = LabelEncoder()\n",
    "seq_enc.fit(np.hstack(df_balanced.seq_ord_pad.values))\n",
    "df_balanced[\"seq_num_pad\"] = df_balanced.seq_ord_pad.map(lambda x: seq_enc.transform(x))\n",
    "print(list(df_balanced.seq_num_pad[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "A popular method for encdoding categorical features that avoids an implicit order is \"one hot encoding\". Again scikit-learn offers tools for this (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder ).<br>\n",
    "<b>Use OneHotEncoder to create column that contains the one hot encoded sequence (use sparse=False)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "oh_enc = OneHotEncoder(sparse=False, categories=\"auto\")\n",
    "oh_enc.fit(np.hstack(df_balanced.seq_num_pad.to_numpy()).reshape(-1, 1))\n",
    "\n",
    "df_balanced[\"seq_oh_pad\"] = df_balanced.seq_num_pad.map(lambda x: oh_enc.transform(x.reshape(-1,1)))\n",
    "\n",
    "print(df_balanced.seq_oh_pad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattened one hot encoding\n",
    "This is a variation of the one hot encoding. Instead of having a one hot vector in each position the vectors are transposed and concatenated (i.e [[0,1],[1,0]] -> [0,1,1,0]).<br>\n",
    "<b>Create column that contains the flattened one hot encoded sequence</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5020,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced[\"seq_oh_flat\"] = df_balanced.seq_oh_pad.map(lambda x: x.flatten())\n",
    "df_balanced.seq_oh_flat[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget the labels\n",
    "We have to encode the labels as well. Again scikit-learn offers several options but we will just use the LabelEncoder again, since we only have two classes (i.e. 0 or 1).<br>\n",
    "<b>Create a column with encoded labels using LabelEncoder (save the encoder in a variable so we can use it for inverse transformation later)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arabidopsis_thaliana300001_SnoR1b        0\n",
       "Arabidopsis_thaliana300003_SnoR10-1      0\n",
       "Arabidopsis_thaliana300006_SnoR101       0\n",
       "Arabidopsis_thaliana300007_SnoR102       0\n",
       "Arabidopsis_thaliana300010_SnoR105       0\n",
       "                                        ..\n",
       "Saccharomyces_cerevisiae300063_snR35     1\n",
       "Saccharomyces_cerevisiae300064_snR5      1\n",
       "Saccharomyces_cerevisiae300065_snR9      1\n",
       "Saccharomyces_cerevisiae300070_snR44     1\n",
       "Saccharomyces_cerevisiae300073_snR191    1\n",
       "Name: lbl_num, Length: 984, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lbl_enc = LabelEncoder()\n",
    "df_balanced[\"lbl_num\"] = lbl_enc.fit_transform(df_balanced.Label)\n",
    "df_balanced.lbl_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Save all of our hard work to a pickle so we can use it in other notebooks<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump(df_balanced, open(\"df_balanced_enc.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test\n",
    "<b>Create a 80/20 train test split, use the encoded labels!</b><br>\n",
    "You can fix the random seed to get a reproducible split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rnd_seed=42\n",
    "xTrain, xTest, yTrain, yTest = train_test_split( df_balanced, df_balanced.lbl_num, test_size=0.2, random_state=rnd_seed )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create First convolutional model\n",
    "Now we are finally ready to create our first model.<br>\n",
    "The model is going to have the following simple architecture:\n",
    "<li>A 1D convolutional layer using an RelU activation with padding=\"same\" (Why?). Let's start with 16 filters of size 7. Use the input_shape parameter </li>\n",
    "<li>A max pooling layer reducing by a factor of 2</li>\n",
    "<li>A Flatten layer</li>\n",
    "<li>A Dense output layer using a sigmoid activation</li>\n",
    "<li>We will use the SGD optimizer with momentum set to 0.9. <i>What is a good choice for the loss function?</i></li><br>\n",
    "\n",
    "Since we want to use GridSearchCV from sklearn later on we will create a function that returns the compiled model (This is needed for the scikit-learn wraper).<br>\n",
    "This function should have parameters for the input_shape, because we are going to test our different encoding schemes.\n",
    "It should also have parameters for the number of filters and their sizes in the first convolutional layer for the grid search. <i>Feel free to insert a print(model.summary()) to get an overview of the model</i><br>We are going to train for 10 epochs using a batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "epochs=10\n",
    "batch_size=32\n",
    "\n",
    "def create_first_model(input_shape, opt=keras.optimizers.SGD(momentum=0.9), c1_filter=16, c1_size=7, verbose=0):\n",
    "    # create the model\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv1D(c1_filter, c1_size, activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPooling1D(2))\n",
    "    model.add(keras.layers.Flatten())    \n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.SGD(momentum=0.9),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different encodings\n",
    "Now we will test the different encoding techniques with the first model. For each test the KerasClassifier wrapper will be used to create the model (https://keras.io/scikit-learn-api/ )\n",
    "<br>The model should then be trained with the training data and a classification report for the predictions on the test data should be generated. You can use th inverse_transform function of the LabelEncoder to have readable labels in the classification report.<br>Use the verbose option while fitting to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model with ASCII encoded sequences\n",
    "<b> Create a KerasClassifier, train the model using the ASCII encoded training data and evaluate the model (numpy.reshape might come in handy)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 1004, 16)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 502, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8032)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 8033      \n",
      "=================================================================\n",
      "Total params: 8,161\n",
      "Trainable params: 8,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "7/7 [==============================] - 0s 739us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      CD-box       0.90      0.93      0.92        99\n",
      "    HACA-box       0.93      0.90      0.91        98\n",
      "\n",
      "    accuracy                           0.91       197\n",
      "   macro avg       0.91      0.91      0.91       197\n",
      "weighted avg       0.91      0.91      0.91       197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_first_model, input_shape=(1004,1), verbose=1)\n",
    "model.fit(np.array(xTrain.seq_ord_pad.to_list()).reshape(-1,1004,1), yTrain.to_numpy(), epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "yPred = model.predict(np.array(xTest.seq_ord_pad.to_list()).reshape(-1,1004,1))\n",
    "print(classification_report( lbl_enc.inverse_transform(yTest.to_list()), lbl_enc.inverse_transform(yPred.ravel())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model with numerically encoded sequences\n",
    "<b> Create a KerasClassifier, train the model using the numerically encoded training data and evaluate the model (numpy.reshape might come in handy)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1004, 16)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 502, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8032)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8033      \n",
      "=================================================================\n",
      "Total params: 8,161\n",
      "Trainable params: 8,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "7/7 [==============================] - 0s 477us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92        99\n",
      "           1       0.96      0.87      0.91        98\n",
      "\n",
      "    accuracy                           0.91       197\n",
      "   macro avg       0.92      0.91      0.91       197\n",
      "weighted avg       0.92      0.91      0.91       197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_first_model, input_shape=(1004,1), verbose=1)\n",
    "model.fit(np.array(xTrain.seq_num_pad.to_list()).reshape(787,1004,1), yTrain.to_numpy(), epochs=epochs, batch_size=batch_size, verbose=0) # start training\n",
    "yPred = model.predict(np.array(xTest.seq_num_pad.to_list()).reshape(-1,1004,1))\n",
    "print(classification_report( yTest.to_list(), yPred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model with one hot encoded sequences\n",
    "<b> Create a KerasClassifier, train the model using the one hot encoded training data and evaluate the model (numpy.reshape might come in handy)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1004, 16)          576       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 502, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8032)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8033      \n",
      "=================================================================\n",
      "Total params: 8,609\n",
      "Trainable params: 8,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "7/7 [==============================] - 0s 39ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        99\n",
      "           1       0.94      0.94      0.94        98\n",
      "\n",
      "    accuracy                           0.94       197\n",
      "   macro avg       0.94      0.94      0.94       197\n",
      "weighted avg       0.94      0.94      0.94       197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_first_model, input_shape=(1004,5), verbose=1)\n",
    "model.fit(np.array(xTrain.seq_oh_pad.to_list()).reshape(787,1004,5), yTrain.to_numpy(), epochs=epochs, batch_size=batch_size, verbose=0) # start training\n",
    "yPred = model.predict(np.array(xTest.seq_oh_pad.to_list()).reshape(-1,1004,5))\n",
    "print(classification_report( yTest.to_list(), yPred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model with flat one hot encoded sequences\n",
    "<b> Create a KerasClassifier, train the model using the flat one hot encoded training data and evaluate the model (numpy.reshape might come in handy)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 5020, 16)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2510, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 40160)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 40161     \n",
      "=================================================================\n",
      "Total params: 40,289\n",
      "Trainable params: 40,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "7/7 [==============================] - 0s 41ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92        99\n",
      "           1       0.97      0.86      0.91        98\n",
      "\n",
      "    accuracy                           0.91       197\n",
      "   macro avg       0.92      0.91      0.91       197\n",
      "weighted avg       0.92      0.91      0.91       197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_first_model, input_shape=(5020,1), verbose=1)\n",
    "model.fit(np.array(xTrain.seq_oh_flat.to_list()).reshape(787,5020,1), yTrain.to_numpy(), epochs=epochs, batch_size=batch_size, verbose=0) # start training\n",
    "yPred = model.predict(np.array(xTest.seq_oh_flat.to_list()).reshape(-1,5020,1))\n",
    "print(classification_report( yTest.to_list(), yPred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Which encoding performed best?<br>\n",
    "Re-run the cells. Are the results stable?<br>\n",
    "Are all CNNs used the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embrace the randomness!\n",
    "In order to better evaluate the impact of the encoding we are going to perform a 5-fold cross validation. For this we are using the StratifiedKFold function from scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) and evaluate the Matthews Correlation Coefficient and the F1 score for each model in each round. <br>\n",
    "<b> Create a DataFrame with a column for each metric for each model.<br> \n",
    "    Create splits from the training dataset using StratifiedKFold.<br>\n",
    "    For each split train the model for all 4 encoding schemes and calculate the MCC for the test data (of the split).<br>\n",
    "    Create a new row for each round in the DataFrame and save the MCCs and F1 scores in the appropriate column<br>\n",
    "    Finally add a row to the DataFrame containing the mean for each model over all rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [  2   3   5   6  10  11  13  14  17  18  22  26  27  28  29  30  33  35\n",
      "  39  40  41  43  44  45  46  47  49  50  52  53  54  55  60  61  62  66\n",
      "  67  68  69  70  71  72  73  76  79  83  85  86  90  96  98  99 100 101\n",
      " 102 105 108 113 114 121 122 124 125 127 129 131 132 133 134 135 138 139\n",
      " 142 143 144 145 146 147 151 152 153 154 160 161 163 166 167 169 170 171\n",
      " 172 173 174 175 177 178 179 186 187 189 190 191 193 195 197 203 204 208\n",
      " 212 213 215 216 218 220 223 225 227 228 231 232 233 236 239 240 244 246\n",
      " 247 249 252 255 256 257 261 263 265 266 267 268 269 272 277 278 279 283\n",
      " 287 293 294 295 298 299 300 302 305 306 307 311 312 314 315 316 317 319\n",
      " 322 323 326 328 330 332 337 339 340 343 345 347 348 349 353 354 357 359\n",
      " 362 365 370 372 373 375 379 382 384 385 388 389 390 393 396 398 401 402\n",
      " 403 404 405 406 407 409 412 413 415 416 417 420 421 422 425 427 431 434\n",
      " 436 437 439 440 444 445 447 450 451 453 455 461 463 464 466 467 468 469\n",
      " 470 474 475 476 478 479 480 483 484 486 488 489 494 496 497 499 504 508\n",
      " 509 511 512 515 516 518 520 522 523 526 529 531 532 533 535 537 539 541\n",
      " 542 543 545 549 551 554 555 556 558 559 560 561 563 565 568 569 571 572\n",
      " 574 576 578 579 582 583 584 586 589 591 592 594 595 596 599 600 602 604\n",
      " 607 608 614 617 618 620 621 622 624 626 627 628 631 635 636 637 641 643\n",
      " 644 646 647 650 651 652 653 658 659 660 661 662 663 665 666 668 669 672\n",
      " 678 679 681 682 684 686 687 688 689 690 691 693 694 695 699 700 701 702\n",
      " 703 707 708 715 718 719 721 726 727 728 729 730 731 736 737 738 740 741\n",
      " 749 752 754 755 757 760 766 767 770 774 775 776 779 780 786] [  0   1   4   7   8   9  12  15  16  19  20  21  23  24  25  31  32  34\n",
      "  36  37  38  42  48  51  56  57  58  59  63  64  65  74  75  77  78  80\n",
      "  81  82  84  87  88  89  91  92  93  94  95  97 103 104 106 107 109 110\n",
      " 111 112 115 116 117 118 119 120 123 126 128 130 136 137 140 141 148 149\n",
      " 150 155 156 157 158 159 162 164 165 168 176 180 181 182 183 184 185 188\n",
      " 192 194 196 198 199 200 201 202 205 206 207 209 210 211 214 217 219 221\n",
      " 222 224 226 229 230 234 235 237 238 241 242 243 245 248 250 251 253 254\n",
      " 258 259 260 262 264 270 271 273 274 275 276 280 281 282 284 285 286 288\n",
      " 289 290 291 292 296 297 301 303 304 308 309 310 313 318 320 321 324 325\n",
      " 327 329 331 333 334 335 336 338 341 342 344 346 350 351 352 355 356 358\n",
      " 360 361 363 364 366 367 368 369 371 374 376 377 378 380 381 383 386 387\n",
      " 391 392 394 395 397 399 400 408 410 411 414 418 419 423 424 426 428 429\n",
      " 430 432 433 435 438 441 442 443 446 448 449 452 454 456 457 458 459 460\n",
      " 462 465 471 472 473 477 481 482 485 487 490 491 492 493 495 498 500 501\n",
      " 502 503 505 506 507 510 513 514 517 519 521 524 525 527 528 530 534 536\n",
      " 538 540 544 546 547 548 550 552 553 557 562 564 566 567 570 573 575 577\n",
      " 580 581 585 587 588 590 593 597 598 601 603 605 606 609 610 611 612 613\n",
      " 615 616 619 623 625 629 630 632 633 634 638 639 640 642 645 648 649 654\n",
      " 655 656 657 664 667 670 671 673 674 675 676 677 680 683 685 692 696 697\n",
      " 698 704 705 706 709 710 711 712 713 714 716 717 720 722 723 724 725 732\n",
      " 733 734 735 739 742 743 744 745 746 747 748 750 751 753 756 758 759 761\n",
      " 762 763 764 765 768 769 771 772 773 777 778 781 782 783 784 785]\n",
      "Finished round 0\n",
      "1 [  0   1   4   7   8   9  12  15  16  19  20  21  23  24  25  31  32  34\n",
      "  36  37  38  42  48  51  56  57  58  59  63  64  65  74  75  77  78  80\n",
      "  81  82  84  87  88  89  91  92  93  94  95  97 103 104 106 107 109 110\n",
      " 111 112 115 116 117 118 119 120 123 126 128 130 136 137 140 141 148 149\n",
      " 150 155 156 157 158 159 162 164 165 168 176 180 181 182 183 184 185 188\n",
      " 192 194 196 198 199 200 201 202 205 206 207 209 210 211 214 217 219 221\n",
      " 222 224 226 229 230 234 235 237 238 241 242 243 245 248 250 251 253 254\n",
      " 258 259 260 262 264 270 271 273 274 275 276 280 281 282 284 285 286 288\n",
      " 289 290 291 292 296 297 301 303 304 308 309 310 313 318 320 321 324 325\n",
      " 327 329 331 333 334 335 336 338 341 342 344 346 350 351 352 355 356 358\n",
      " 360 361 363 364 366 367 368 369 371 374 376 377 378 380 381 383 386 387\n",
      " 391 392 394 395 397 399 400 408 410 411 414 418 419 423 424 426 428 429\n",
      " 430 432 433 435 438 441 442 443 446 448 449 452 454 456 457 458 459 460\n",
      " 462 465 471 472 473 477 481 482 485 487 490 491 492 493 495 498 500 501\n",
      " 502 503 505 506 507 510 513 514 517 519 521 524 525 527 528 530 534 536\n",
      " 538 540 544 546 547 548 550 552 553 557 562 564 566 567 570 573 575 577\n",
      " 580 581 585 587 588 590 593 597 598 601 603 605 606 609 610 611 612 613\n",
      " 615 616 619 623 625 629 630 632 633 634 638 639 640 642 645 648 649 654\n",
      " 655 656 657 664 667 670 671 673 674 675 676 677 680 683 685 692 696 697\n",
      " 698 704 705 706 709 710 711 712 713 714 716 717 720 722 723 724 725 732\n",
      " 733 734 735 739 742 743 744 745 746 747 748 750 751 753 756 758 759 761\n",
      " 762 763 764 765 768 769 771 772 773 777 778 781 782 783 784 785] [  2   3   5   6  10  11  13  14  17  18  22  26  27  28  29  30  33  35\n",
      "  39  40  41  43  44  45  46  47  49  50  52  53  54  55  60  61  62  66\n",
      "  67  68  69  70  71  72  73  76  79  83  85  86  90  96  98  99 100 101\n",
      " 102 105 108 113 114 121 122 124 125 127 129 131 132 133 134 135 138 139\n",
      " 142 143 144 145 146 147 151 152 153 154 160 161 163 166 167 169 170 171\n",
      " 172 173 174 175 177 178 179 186 187 189 190 191 193 195 197 203 204 208\n",
      " 212 213 215 216 218 220 223 225 227 228 231 232 233 236 239 240 244 246\n",
      " 247 249 252 255 256 257 261 263 265 266 267 268 269 272 277 278 279 283\n",
      " 287 293 294 295 298 299 300 302 305 306 307 311 312 314 315 316 317 319\n",
      " 322 323 326 328 330 332 337 339 340 343 345 347 348 349 353 354 357 359\n",
      " 362 365 370 372 373 375 379 382 384 385 388 389 390 393 396 398 401 402\n",
      " 403 404 405 406 407 409 412 413 415 416 417 420 421 422 425 427 431 434\n",
      " 436 437 439 440 444 445 447 450 451 453 455 461 463 464 466 467 468 469\n",
      " 470 474 475 476 478 479 480 483 484 486 488 489 494 496 497 499 504 508\n",
      " 509 511 512 515 516 518 520 522 523 526 529 531 532 533 535 537 539 541\n",
      " 542 543 545 549 551 554 555 556 558 559 560 561 563 565 568 569 571 572\n",
      " 574 576 578 579 582 583 584 586 589 591 592 594 595 596 599 600 602 604\n",
      " 607 608 614 617 618 620 621 622 624 626 627 628 631 635 636 637 641 643\n",
      " 644 646 647 650 651 652 653 658 659 660 661 662 663 665 666 668 669 672\n",
      " 678 679 681 682 684 686 687 688 689 690 691 693 694 695 699 700 701 702\n",
      " 703 707 708 715 718 719 721 726 727 728 729 730 731 736 737 738 740 741\n",
      " 749 752 754 755 757 760 766 767 770 774 775 776 779 780 786]\n",
      "Finished round 1\n",
      "        ord_mcc    ord_f1   num_mcc    num_f1    oh_mcc     oh_f1   foh_mcc  \\\n",
      "Round                                                                         \n",
      "0      0.665009  0.833333  0.809782  0.907317  0.838255  0.920398  0.751699   \n",
      "1      0.766513  0.885572  0.800645  0.893048  0.878262  0.940000  0.832915   \n",
      "mean   0.715761  0.859453  0.805213  0.900183  0.858258  0.930199  0.792307   \n",
      "\n",
      "         foh_f1  \n",
      "Round            \n",
      "0      0.880184  \n",
      "1      0.918114  \n",
      "mean   0.899149  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "#to save scores\n",
    "df_scores = pd.DataFrame(columns=[\"ord_mcc\", \"ord_f1\",\n",
    "                                  \"num_mcc\", \"num_f1\",\n",
    "                                  \"oh_mcc\", \"oh_f1\",\n",
    "                                  \"foh_mcc\", \"foh_f1\"])\n",
    "df_scores.index.name = \"Round\"\n",
    "\n",
    "# define 5-fold cross validation test \n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(xTrain, yTrain)):\n",
    "    #print (i,train_index, test_index)\n",
    "    \n",
    "    # get the splits\n",
    "    X_train, X_test = xTrain.iloc[train_index], xTrain.iloc[test_index]\n",
    "    y_train, y_test = yTrain.iloc[train_index], yTrain.iloc[test_index]\n",
    "    \n",
    "    # train ascii model\n",
    "    model_ord = KerasClassifier(build_fn=create_first_model, input_shape=(1004,1), verbose=0)\n",
    "    model_ord.fit(np.array(X_train.seq_ord_pad.to_list()).reshape(-1,1004,1), y_train.to_numpy(), epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    yPred_ord = model_ord.predict(np.array(X_test.seq_ord_pad.to_list()).reshape(-1,1004,1))\n",
    "        \n",
    "    # train num model\n",
    "    model_num = KerasClassifier(build_fn=create_first_model, input_shape=(1004,1), verbose=0)\n",
    "    model_num.fit(np.array(X_train.seq_num_pad.to_list()).reshape(-1,1004,1), y_train.to_numpy(), epochs=epochs, batch_size=batch_size, verbose=0) # start training\n",
    "    yPred_num = model_num.predict(np.array(X_test.seq_num_pad.to_list()).reshape(-1,1004,1))\n",
    "    \n",
    "    # train one hot \n",
    "    model_oh = KerasClassifier(build_fn=create_first_model, input_shape=(1004,5), verbose=0)\n",
    "    model_oh.fit(np.array(X_train.seq_oh_pad.to_list()).reshape(-1,1004,5), y_train.to_numpy(), epochs=epochs, batch_size=batch_size, verbose=0) # start training\n",
    "    yPred_oh = model_oh.predict(np.array(X_test.seq_oh_pad.to_list()).reshape(-1,1004,5))\n",
    "    \n",
    "    # train flat one hot\n",
    "    model_foh = KerasClassifier(build_fn=create_first_model, input_shape=(5020,1), verbose=0)\n",
    "    model_foh.fit(np.array(X_train.seq_oh_flat.to_list()).reshape(-1,5020,1), y_train.to_numpy(), epochs=epochs, batch_size=batch_size, verbose=0) # start training\n",
    "    yPred_foh = model_foh.predict(np.array(X_test.seq_oh_flat.to_list()).reshape(-1,5020,1))\n",
    "    \n",
    "    # save predictions\n",
    "    df_scores.loc[i] = [matthews_corrcoef(y_test.to_list(), yPred_ord), f1_score(y_test.to_list(), yPred_ord),\n",
    "                        matthews_corrcoef(y_test.to_list(), yPred_num), f1_score(y_test.to_list(), yPred_num),\n",
    "                        matthews_corrcoef(y_test.to_list(), yPred_oh), f1_score(y_test.to_list(), yPred_oh),\n",
    "                        matthews_corrcoef(y_test.to_list(), yPred_foh), f1_score(y_test.to_list(), yPred_foh)]\n",
    "    \n",
    "    print(\"Finished round {}\".format(i))\n",
    "    \n",
    "df_scores.loc['mean'] = df_scores.mean()\n",
    "print(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the encoding can have an impact on the performance of our model.<br>\n",
    "The one hot encoding seems to produce the best results so far, so we are going to concentrate on this encoding scheme when evaluating our next model.<br>\n",
    "The flattened one hot encoding also performed almost as good but it also increased the trainable parameters of our model (Why should that be a concern?)<br>\n",
    "<b>Let's move on to the next notebook \"03_second_CNN\" with a slightly more complex model<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
